{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Dropout, Dense, Activation, CuDNNLSTM\n",
    "from keras.layers import LSTM, Bidirectional,Input\n",
    "from keras.layers import concatenate\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = json.load(open('train-v2.0.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = json.load(open('dev-v2.0.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [word for word in words if word and word not in stops]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': [{'answer_start': 182, 'text': 'Polish and French'}],\n",
       " 'id': '56cbd2356d243a140015ed66',\n",
       " 'is_impossible': False,\n",
       " 'question': \"What was Frédéric's nationalities?\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['data'][1]['paragraphs'][0]['qas'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = []\n",
    "questions = []\n",
    "\n",
    "starts = []\n",
    "ends = []\n",
    "\n",
    "for instance in train['data']:\n",
    "    for paragraph in instance['paragraphs']:\n",
    "        context = paragraph['context']\n",
    "        \n",
    "        for qas in paragraph['qas']:\n",
    "            if qas['is_impossible']:\n",
    "                continue\n",
    "            question = qas['question']\n",
    "            for answer in qas['answers']:\n",
    "                start = answer['answer_start']\n",
    "                end = start + len(answer['text'])\n",
    "                contexts.append(normalize(context))\n",
    "                questions.append(normalize(question))\n",
    "                starts.append(start)\n",
    "                ends.append(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts_dev = []\n",
    "questions_dev = []\n",
    "\n",
    "starts_dev = []\n",
    "ends_dev = []\n",
    "\n",
    "for instance in dev['data']:\n",
    "    for paragraph in instance['paragraphs']:\n",
    "        context = paragraph['context']\n",
    "        \n",
    "        for qas in paragraph['qas']:\n",
    "            if qas['is_impossible']:\n",
    "                continue\n",
    "            question = qas['question']\n",
    "            \n",
    "            for answer in qas['answers']:\n",
    "                \n",
    "                start = answer['answer_start']\n",
    "                end = start + len(answer['text'])\n",
    "                contexts_dev.append(normalize(context))\n",
    "                questions_dev.append(normalize(question))\n",
    "                starts_dev.append(start)\n",
    "                ends_dev.append(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "\n",
    "for context in contexts:\n",
    "    vocab.update(context)\n",
    "\n",
    "for question in questions:\n",
    "    vocab.update(question)\n",
    "\n",
    "id2word = {i:word for i, word in enumerate(vocab)}\n",
    "word2id = {word:i for i, word in id2word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts_le = [[word2id[word] for word in context] for context in contexts]\n",
    "max_len = max([len(c) for c in contexts])\n",
    "\n",
    "X_train_context = pad_sequences(contexts_le, max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts_le_dev = [[word2id.get(word, 0) for word in context] for context in contexts_dev]\n",
    "\n",
    "X_dev_context = pad_sequences(contexts_le_dev, max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86821, 410)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_context.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_le = [[word2id[word] for word in question] for question in questions]\n",
    "max_len_q = max([len(c) for c in questions])\n",
    "\n",
    "X_train_question = pad_sequences(questions_le, max_len_q, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_le_dev = [[word2id.get(word, 0) for word in question] for question in questions_dev]\n",
    "X_dev_question = pad_sequences(questions_le_dev, max_len_q, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_vector_length = 50\n",
    "\n",
    "max_span_begin = np.max(starts)\n",
    "max_span_end = np.max(ends)\n",
    "batch = 64\n",
    "\n",
    "\n",
    "# slice of data to be used as one epoch training on full data is expensive\n",
    "slce = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "starts = np.array(starts)\n",
    "ends = np.array(ends)\n",
    "\n",
    "starts_dev = np.array(starts_dev)\n",
    "ends_dev = np.array(ends_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "context_input (InputLayer)      (None, 410)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ques_input (InputLayer)         (None, 31)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 410, 50)      5319400     context_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 31, 50)       5319400     ques_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 410, 100)     40800       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 31, 100)      40800       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 410, 100)     0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 31, 100)      0           bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 441, 100)     0           dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 50)           60800       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3126)         159426      bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 3136)         159936      bidirectional_6[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 11,100,562\n",
      "Trainable params: 11,100,562\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# model1\n",
    "context_input = Input(shape=(max_len, ), dtype='int32', name='context_input')\n",
    "x = Embedding(input_dim=vocab_size, output_dim=50,\n",
    "              input_length=max_len)(context_input)\n",
    "lstm_out = Bidirectional(CuDNNLSTM(50, return_sequences=True), merge_mode='concat')(x)\n",
    "drop_1 = Dropout(0.1)(lstm_out)\n",
    "\n",
    "# model2\n",
    "ques_input = Input(shape=(max_len_q, ), dtype='int32', name='ques_input')\n",
    "x = Embedding(input_dim=vocab_size, output_dim=50,\n",
    "              input_length=max_len_q)(ques_input)\n",
    "lstm_out = Bidirectional(CuDNNLSTM(50, return_sequences=True), merge_mode='concat')(x)\n",
    "drop_2 = Dropout(0.1)(lstm_out)\n",
    "\n",
    "# merger model\n",
    "merge_layer = concatenate([drop_1, drop_2], axis=1)\n",
    "biLSTM = Bidirectional(CuDNNLSTM(50), merge_mode='mul')(merge_layer)\n",
    "drop_3 =  Dropout(0.1)(biLSTM)\n",
    "softmax_1 = Dense(max_span_begin, activation='softmax')(biLSTM)\n",
    "softmax_2 = Dense(max_span_end, activation='softmax')(biLSTM)\n",
    "\n",
    "model = Model(inputs=[context_input, ques_input], outputs=[softmax_1, softmax_2])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No data provided for \"dense_3\". Need data for each key in: ['dense_3', 'dense_4']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             ]\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             ]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dense_3'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b669cf0d571d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m model.fit(training_data[0], training_data[1], batch_size=128, epochs=10,\n\u001b[0;32m---> 13\u001b[0;31m           validation_data=(validation_data[0], validation_data[1]))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     76\u001b[0m             raise ValueError('No data provided for \"' + e.args[0] +\n\u001b[1;32m     77\u001b[0m                              \u001b[0;34m'\". Need data '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                              'for each key in: ' + str(names))\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No data provided for \"dense_3\". Need data for each key in: ['dense_3', 'dense_4']"
     ]
    }
   ],
   "source": [
    "validation_data=({'context_input': X_dev_context,\n",
    "                  'ques_input':X_dev_question}, \n",
    "                 {'dense_1': starts_dev,\n",
    "                  'dense_2': ends_dev})\n",
    "\n",
    "\n",
    "training_data=({'context_input': X_train_context,\n",
    "                'ques_input':X_train_question}, \n",
    "                 {'dense_1': starts,\n",
    "                  'dense_2': ends})\n",
    "\n",
    "model.fit(training_data[0], training_data[1], batch_size=128, epochs=10,\n",
    "          validation_data=(validation_data[0], validation_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kathmandu',\n",
       " 'metropolitan',\n",
       " 'city',\n",
       " 'kmc',\n",
       " 'order',\n",
       " 'promote',\n",
       " 'international',\n",
       " 'relations',\n",
       " 'established',\n",
       " 'international',\n",
       " 'relations',\n",
       " 'secretariat',\n",
       " 'irc',\n",
       " \"kmc's\",\n",
       " 'first',\n",
       " 'international',\n",
       " 'relationship',\n",
       " 'established',\n",
       " '1975',\n",
       " 'city',\n",
       " 'eugene',\n",
       " 'oregon',\n",
       " 'united',\n",
       " 'states',\n",
       " 'activity',\n",
       " 'enhanced',\n",
       " 'establishing',\n",
       " 'formal',\n",
       " 'relationships',\n",
       " '8',\n",
       " 'cities',\n",
       " 'motsumoto',\n",
       " 'city',\n",
       " 'japan',\n",
       " 'rochester',\n",
       " 'usa',\n",
       " 'yangon',\n",
       " 'formerly',\n",
       " 'rangoon',\n",
       " 'myanmar',\n",
       " \"xi'an\",\n",
       " \"people's\",\n",
       " 'republic',\n",
       " 'china',\n",
       " 'minsk',\n",
       " 'belarus',\n",
       " 'pyongyang',\n",
       " 'democratic',\n",
       " 'republic',\n",
       " 'korea',\n",
       " \"kmc's\",\n",
       " 'constant',\n",
       " 'endeavor',\n",
       " 'enhance',\n",
       " 'interaction',\n",
       " 'saarc',\n",
       " 'countries',\n",
       " 'international',\n",
       " 'agencies',\n",
       " 'many',\n",
       " 'major',\n",
       " 'cities',\n",
       " 'world',\n",
       " 'achieve',\n",
       " 'better',\n",
       " 'urban',\n",
       " 'management',\n",
       " 'developmental',\n",
       " 'programs',\n",
       " 'kathmandu']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
