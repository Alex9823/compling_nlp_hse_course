{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбиение текста на предложения (Определение границ предложений)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Текст в стандартной форме"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если в тексте есть знаки препинания, соблюдается регистр и мало ошибок -  найти границы предложений для большинства случаев будет достаточно просто. Большинство предложений заканчиваются на точку, вопросительный или восклицательные знаки, после которых идет какой-то отступ и другое слово с заглавной буквы. \n",
    "\n",
    "Да иногда будут попадаться аббревиатуры и сокращения или случаи, когда пробела после точки не стоит, но для задач, где не нужна точность - это не будет сильной проблемой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В nltk есть такая функция. Так как в русском и английском знаки препинания по большей части одинаковые - можно смело ей пользоваться прямо из коробки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk import sent_tokenize\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files= ['../data/'+file for file in os.listdir('../data')]\n",
    "data = pd.concat([pd.read_json(file, lines=True) for file in files], axis=0, ignore_index=True)['content'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для примера возьмем совсем немного текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Многие интересуются, зачем нужна «Яблоку» молодежная фракция?',\n",
       " 'Основной задачей «Молодежного «Яблока» является привлечение молодых людей к участию в выборах и деятельности партии.',\n",
       " '«Молодежное «Яблоко» работает более чем в 10 регионах.',\n",
       " 'Единого руководства у нас нет, но мы стараемся координировать свою деятельность и периодически проводим акции на федеральном уровне.',\n",
       " 'Мы ведем борьбу с обязательным воинским призывом.',\n",
       " 'Военный – это профессия, а не обязанность.',\n",
       " 'Молодые люди вправе сами распоряжаться своей жизнью и не терять целый год, отдавая государству «долг», который они у него не занимали.',\n",
       " 'По мнению одного из ведущих специалистов в области оборонной политики Алексея Арбатова, переход на контрактную армию будет стоить лишь 2% военного бюджета.',\n",
       " 'Также на федеральном уровне «Молодежное «Яблоко» проводило акции за освобождение политзаключенных и против вмешательства России во внутреннюю политику Украины.',\n",
       " 'Расскажу о московских активистах.',\n",
       " 'Виктору Петрунину – 19 лет, он пришел к нам больше чем полтора года назад, когда только переехал в Москву.',\n",
       " 'Вся его семья придерживается демократических взглядов, а Виктора с раннего детства интересовала политика.',\n",
       " 'Виктор всегда очень критически относился к власти, но после Крыма, войны в Украине и убийства Бориса Немцова понял, что не может оставаться в стороне.',\n",
       " 'Заканчивая школу, он решил для себя, что должен присоединиться к близкой ему идеологически силе и добиваться смены власти в России.',\n",
       " '17-летняя Дарья Новичкова рассказывает, что огромное влияние на нее оказал ее преподаватель по обществознанию, который всегда трезво оценивал политическую ситуацию в стране.',\n",
       " 'Его искренность и неподдельная заинтересованность во многих злободневных вопросах помогли Дарье научиться фильтровать материал, который публикуется в различных СМИ.',\n",
       " 'Через некоторое время она поняла, что пассивное наблюдение за всем происходящим – самая невыгодная позиция из всех возможных, а потому необходимо каким-то образом действовать, лучше всего в команде единомышленников.',\n",
       " 'Затем, вдохновившись примером своего ровесника, который рассказал Дарье про «Молодежное «Яблоко», она решила к нам вступить.',\n",
       " 'В московском «Молодежном «Яблоке» действует дискуссионный клуб, в рамках него проходят лекции и кинопоказы на самые разные темы – от наступления консерватизма, обсуждения социально-либеральной альтернативы до ситуации в российской экономике.',\n",
       " 'Недавно один из участников круглого стола сказал нам, что никогда не присутствовал на мероприятиях, где имеет место столь открытая дискуссия, в которой может принять участие любой гость.',\n",
       " 'Петербургское «Молодежное «Яблоко» серьезно работает по городской повестке.',\n",
       " 'А в ноябре прошлого года они отправились в Карелию, чтобы провести акцию за свободу Ильдара Дадина рядом с колонией, где он тогда содержался, за что были задержаны, а судебный процесс по административному правонарушению, в котором обвинили активистов, до сих пор не завершился.',\n",
       " 'Ставропольцы недавно провели акции против декриминализации домашнего насилия, фотографии которой облетели Интернет.',\n",
       " '8 марта они организовали театрализованную акцию, чтобы напомнить, что этот «праздник» изначально появился как день борьбы за права женщин, но сегодня в России это пародия на его истинный смысл – женщин поздравляют с тем, что они «украшение» и «слабый пол».',\n",
       " 'Североосетинское отделение выступает против деятельности завода «Электроцинк», который наносит тяжелый ущерб окружающей среде.',\n",
       " 'Мы участвуем и в общепартийных мероприятиях, становимся кандидатами на выборах различного уровня.',\n",
       " 'В 2018 году наши активисты намерены участвовать в главной предвыборной кампании страны – выборах президента.',\n",
       " 'У «Яблока» есть свой кандидат – основатель партии Григорий Явлинский.',\n",
       " 'Его последовательность и неготовность разменивать принципы на проценты заслуживают уважения и поддержки.',\n",
       " 'Есть и проблемы.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[re.sub('[\\n\\t]', ' ', x) for x in sent_tokenize(data[0])[:30]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ни одной ошибки!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если уточнить разбиения все-таки нужно, есть способ обучить токенизатор на имеющихся текстах или даже добавить туда спорные случаи вручную.\n",
    "\n",
    "Про то как алгоритм обучается можно почитать тут  - \n",
    "В двух словах: считаются частотные аббревиатуры, которые потом не используются как разделители.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train('\\n'.join(data))\n",
    " \n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Список выученных сокращений можно достать вот так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'с.г', 'куб', 'н.а', 'я»', 'м»', 'пер', 'тыс', 'кв', 'см', 'т.д', 'долл', 'ю.н', 'г', 'аэс', 'др', 's', 'пл', 'стр', 'н.к', 'б', 'д', 'руб'}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer._params.abbrev_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы удобнее просматривать окончания можно отрезать все до последних 10 символов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['е (Техас).',\n",
       " 'оехали 15.',\n",
       " 'ить заезд.',\n",
       " ' Росберга.',\n",
       " 'Риккъярдо.',\n",
       " 'е удалась.',\n",
       " 'це заезда.',\n",
       " ' досрочно.',\n",
       " ' Marussia.',\n",
       " ' Абу-Даби.',\n",
       " 'авершению.',\n",
       " 'ио Переса.',\n",
       " 'айкконена.',\n",
       " 'до боксов.',\n",
       " 'щей гонки.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[re.sub('[\\n\\t]', ' ', x)[-10:] for x in tokenizer.tokenize(data[5])[:15]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну и в него же напрямую можно что-то добавить (без учета регистра и без точки на конце)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# можно сразу добавить все числа и сокращения имен и отчеств\n",
    "tokenizer._params.abbrev_types.add('15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['е (Техас).',\n",
       " 'ить заезд.',\n",
       " ' Росберга.',\n",
       " 'Риккъярдо.',\n",
       " 'е удалась.',\n",
       " 'це заезда.',\n",
       " ' досрочно.',\n",
       " ' Marussia.',\n",
       " ' Абу-Даби.',\n",
       " 'авершению.',\n",
       " 'ио Переса.',\n",
       " 'айкконена.',\n",
       " 'до боксов.',\n",
       " 'щей гонки.',\n",
       " 'кий сход).']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[re.sub('[\\n\\t]', ' ', x)[-10:] for x in tokenizer.tokenize(data[5])[:15]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После нескольких итераций ручной подкрутки может возникнуть желание более точно измерить качество разбиений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как это можно измерить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно взять какой-то текст и разбить его руками. А потом разбить этот ж текст моделью и сравнить длины полученных списков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = tokenizer.tokenize(data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['В Ленинском районном суде продолжаются слушания по делу экс-депутата Думы Владивостока Зинаиды Ким и бывшего председателя избирательного участка № 522 Елены Футиной, которых обвиняют в сговоре и фальсификациях результатов на выборах на сентябрьских выборах 2016 года.',\n",
       " 'Напомним, 18 сентября 2017 года местные журналисты сняли на видео, как Ким, будучи кандидатом по спискам в Законодательное Собрание Приморского края, выдавала молодым людям открепительные, возила их голосовать на участок, где уже знали о предстоящем визите.',\n",
       " 'В качестве вознаграждения избирателям предлагалось по 500 рублей.',\n",
       " 'Перед началом судебного процесса Зинаида Ким разговаривала с журналистами на повышенных тонах и обзывая, доказывала, что видео – монтаж.',\n",
       " 'Адвокаты представили вниманию участников процесса характеристику подсудимой, составленную руководителями Всероссийской общественной организации «Боевое братство (Приморье)», членом которого является подсудимая.',\n",
       " 'Выяснилось, что у Зинаиды Ким – богатый наградной список: есть, например, памятные знаки к 65 – и 70-летию Победы в Великой Отечественной войне, к 25-летию вывода советских войск из Афганистана, 20-летию МЧС и многие другие.',\n",
       " 'В ходе заседания адвокат Алексей Клецкин попросил судью назначить дополнительную комплексную экспертизу видео – и аудиозаписей, представленных в процессе в качестве основных доказательств вины Ким и Футиной, хотя прежде защитники не высказывали сомнений в подлинности экспертизы.',\n",
       " 'Государственный обвинитель прошение защитников назвал необоснованным, судья поддержал позицию представителя прокуратуры.',\n",
       " 'На предшествующих судебных заседаниях по делу Зинаиды Ким и Елены Футиной были допрошены свидетели – бывший главный редактор издания VL.RU Иван Федотов, бывший журналист Маргарита Бабченко.',\n",
       " 'Зинаида Ким заявила в суде, что журналист, выявивший противоправные действия, предлагал ей закрыть дело за вознаграждение в 500 000 рублей.',\n",
       " 'Иван Федотов объяснил ситуацию: он отправил на разговор вместо себя своего коллегу.',\n",
       " 'То, что она встречалась с другим сотрудником редакции, Зинаида Ким поняла только на судебном заседании.',\n",
       " 'Свидетель, внештатный видеограф информационного агентства Даниила Губарев, рассказал суду о работе на сайте, о своих профессиональных обязанностям.',\n",
       " 'Он подтвердил, что никаких манипуляций с видео на мобильном телефоне он не проводил.',\n",
       " 'На компьютере, из отснятых корреспондентами материалов, он смонтировал ролик: весь монтаж заключался в склейке фрагментов и наложении субтитров.',\n",
       " 'По словам свидетеля, в итоговом видео убрали только «грязные» кадры – то есть те, на которых ничего не происходит.',\n",
       " 'На экспертизу видеодоказательств и другие следственные процедуры понадобилось почти девять месяцев и пять депутатских запросов в СУ СК по Приморью.',\n",
       " 'За фальсификацию избирательных документов или документов референдума Ким грозит штраф в размере от 100 тыс. до 300 тыс. рублей или в размере заработной платы, или иного дохода за период до двух лет, либо принудительные работы на срок до четырех лет, либо лишение свободы на тот же срок.',\n",
       " 'Владивосток']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_split = sent_tokenize(data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold length -  19\n",
      "My split -  21\n"
     ]
    }
   ],
   "source": [
    "print('Gold length - ', len(gold))\n",
    "print('My split - ', len(my_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Понятно, что это не очень хороший способ (его можно обмануть), но что-то он покажет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можно привести списки к множествам и посчитать пересечения - одинаковые предложения будут считать совпадающими элементами.\n",
    "\n",
    "Тут нужно быть аккуратным, так как модель может отрезать один лишний символ и это уже будет считать промахом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8181818181818182"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# добавим нормировку на длину объединения\n",
    "len(set(gold) & set(my_split)) / len(set(gold) | set(my_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нем использовалась такая метрика - берем идеальное предложение и пробуем разбить его нашей моделью - если  разбивается, считает этой ошибкой, если нет - правильным ответом. \n",
    "Потом берем два идеальных предложения - склеиваем их пробельным символом и пробуем разбить нашей моделью. Если разбивает на два нужных предложения - все правильно.\n",
    "\n",
    "Таким образом мы вычисляем tp fp fn для расчета точности, полноты и f1 меры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.9714285714285714\n",
      "Recall -  0.9444444444444444\n",
      "F1 -  0.9577464788732395\n"
     ]
    }
   ],
   "source": [
    "tp = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "for sent in gold:\n",
    "    if len(sent_tokenize(sent)) == 1:\n",
    "        tp += 1\n",
    "    else:\n",
    "        fp += 1\n",
    "\n",
    "for i in range(len(gold)-1):\n",
    "    sent1, sent2 = gold[i], gold[i+1]\n",
    "    sent = ' '.join([sent1, sent2])\n",
    "    if len(sent_tokenize(sent)) == 2:\n",
    "        tp += 1\n",
    "    else:\n",
    "        fn += 1\n",
    "\n",
    "precision = (tp/(tp+fp))\n",
    "recall = (tp/(tp+fn))\n",
    "f1 = 2*(precision*recall)/(precision+recall)\n",
    "print('Precision - ', precision)\n",
    "print('Recall - ', recall)\n",
    "print('F1 - ', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такую метрики использовали авторы вот этого токенизатора для русского - https://github.com/deepmipt/ru_sentence_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно придумать и что-то посложнее. \n",
    "\n",
    "Перейдем к индексам символов в тексте. Теперь нам нужно для каждого символа предсказать является ли он разбивающим или нет. Потом можно применять стандартные метрики качества классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 267),\n",
       " (268, 525),\n",
       " (526, 591),\n",
       " (592, 728),\n",
       " (729, 939),\n",
       " (940, 1164),\n",
       " (1166, 1445),\n",
       " (1446, 1566),\n",
       " (1568, 1757),\n",
       " (1758, 1897),\n",
       " (1898, 1981),\n",
       " (1982, 2085),\n",
       " (2086, 2233),\n",
       " (2234, 2318),\n",
       " (2319, 2463),\n",
       " (2464, 2578),\n",
       " (2579, 2726),\n",
       " (2727, 3013),\n",
       " (3015, 3026)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.span_tokenize(data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = [0 for i in range(len(data[3]))]\n",
    "for span in tokenizer.span_tokenize(data[3]):\n",
    "    gold[span[1]-1] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_tokenize(data[3])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_split = [0 for i in range(len(data[3]))]\n",
    "index = 0\n",
    "for sent in sent_tokenize(data[3]):\n",
    "    index += len(sent)\n",
    "    my_split[index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      3007\n",
      "           1       0.05      0.05      0.05        19\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      3026\n",
      "   macro avg       0.52      0.52      0.52      3026\n",
      "weighted avg       0.99      0.99      0.99      3026\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(gold, my_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё можно выписать индексы всех разбивающих символов и рассматривать это как строку. Между двумя строками (идеальной и той, что выдала модель) можно посчитать edit distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editdistance.eval([1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = []\n",
    "my_split = []\n",
    "for span in tokenizer.span_tokenize(data[3]):\n",
    "    gold.append(span[1])\n",
    "\n",
    "index = 0\n",
    "for sent in sent_tokenize(data[3]):\n",
    "    index += len(sent)\n",
    "    my_split.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editdistance.eval(gold, my_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот тут можно почитать про другие метрики (и их сравнение) - http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.417.8097&rep=rep1&type=pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Нестандартный текст"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бывает, что текст приходит откуда-то ещё (например, с text2speech модуля) сплошняком без каких-либо знаков препинания и регистров. Либо в нём столько неточностей, что стандартные методы ошибаются на каждом втором слове (например, при переводе пдфа в текст)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом случае нужно как-то учитывать смысл написанного. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этого хорошо подойдут рекурентные нейронные сети. \n",
    "\n",
    "Обучающую выборку собрать из обычных текстов, разделенных вручную или автоматически, убрав всю пунктуацию и привидя их к нижнему регистру. Ну или сделать то же самое с предложениями из какого-нибудь синтагруса.\n",
    "\n",
    "Саму задачу можно рассматривать как задачу классификации. К целым предложениям можно приписать положительный класс (т.е. после последнего слова нужно поставить разделитель), а для подбора отрицательного класса, можно нарезать предложения на части и каждой из них приписать отрицательный класс (т.е. разделителя не нужно). \n",
    "\n",
    "Например, предложение \"вчера кажется бы снег\" преборазуется в:\n",
    "\n",
    "\"вчера кажется был снег\" - 1\n",
    "\"вчера кажется был\" - 0\n",
    "\"кажется был\" - 0\n",
    "\"вчера кажется\" - 0\n",
    "\n",
    "Вообще можно и положительный класс пополнить таким методом (выбрость первое слово, например), но лучше этого не делать. Потому что предложения бывают разные и не всегда можно что-то обросить без потери смысла. Ну и в нулевой класс могут попасть нормальные предложения (был снег, например). Но можно предположиться, что такое будет случаться не часто."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем на небольшом количестве предложений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы ещё ничего не знаете про нейронные сети или знаете очень мало, не пугайтесь. Попробуйте запустить мой код и посмотреть, что он делает. Попробуйте что-то поменять и посмотреть что изменится. Ну и потом уже можете пойти почитать про то, как оно устроено. Такой top-down (от практики к теории) используют в fast.ai для обучения нейронным сетям и это работает! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import numpy as np\n",
    "punct=punctuation+'«…»'\n",
    "def normalize(text):\n",
    "    tokens = [word.strip(punct) for word in text.lower().split()]\n",
    "    tokens = [word for word in tokens if word]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['в',\n",
       " 'ленинском',\n",
       " 'районном',\n",
       " 'суде',\n",
       " 'продолжаются',\n",
       " 'слушания',\n",
       " 'по',\n",
       " 'делу',\n",
       " 'экс-депутата',\n",
       " 'думы']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(tokenizer.tokenize(data[3])[0])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "files= ['../data/'+file for file in os.listdir('../data')]\n",
    "data = pd.concat([pd.read_json(file, lines=True) for file in files], axis=0, ignore_index=True)['content'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_sents = []\n",
    "for text in data:\n",
    "    good_sents += [normalize(sent) for sent in tokenizer.tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49482"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(good_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sents = []\n",
    "for sent in good_sents:\n",
    "    right_half = sent[:len(sent) // 2]\n",
    "    left_half = sent[len(sent) // 2:]\n",
    "    bad_sents.append(right_half)\n",
    "    bad_sents.append(left_half)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98964"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bad_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['многие',\n",
       "  'интересуются',\n",
       "  'зачем',\n",
       "  'нужна',\n",
       "  'яблоку',\n",
       "  'молодежная',\n",
       "  'фракция'],\n",
       " ['основной',\n",
       "  'задачей',\n",
       "  'молодежного',\n",
       "  'яблока',\n",
       "  'является',\n",
       "  'привлечение',\n",
       "  'молодых',\n",
       "  'людей',\n",
       "  'к',\n",
       "  'участию',\n",
       "  'в',\n",
       "  'выборах',\n",
       "  'и',\n",
       "  'деятельности',\n",
       "  'партии'],\n",
       " ['молодежное', 'яблоко', 'работает', 'более', 'чем', 'в', '10', 'регионах'],\n",
       " ['единого',\n",
       "  'руководства',\n",
       "  'у',\n",
       "  'нас',\n",
       "  'нет',\n",
       "  'но',\n",
       "  'мы',\n",
       "  'стараемся',\n",
       "  'координировать',\n",
       "  'свою',\n",
       "  'деятельность',\n",
       "  'и',\n",
       "  'периодически',\n",
       "  'проводим',\n",
       "  'акции',\n",
       "  'на',\n",
       "  'федеральном',\n",
       "  'уровне']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_sents[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = good_sents + bad_sents\n",
    "target = [1 for i in range(len(good_sents))] + [0 for i in range(len(bad_sents))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for sent in sents:\n",
    "    vocab.update(sent)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {i:word for i, word in enumerate(vocab)}\n",
    "word2id = {word:i for i, word in id2word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_ids = []\n",
    "\n",
    "for sent in sents:\n",
    "    sents_ids.append([word2id[word] for word in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[23190, 88340, 10384, 46060, 17400, 90609, 8224],\n",
       " [95460,\n",
       "  62076,\n",
       "  32169,\n",
       "  90967,\n",
       "  17680,\n",
       "  3293,\n",
       "  57267,\n",
       "  72385,\n",
       "  84516,\n",
       "  80919,\n",
       "  14394,\n",
       "  78188,\n",
       "  50636,\n",
       "  17437,\n",
       "  83921],\n",
       " [94366, 22076, 40414, 47640, 53800, 14394, 37841, 39499]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_ids[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM, Embedding, Dense, Dropout\n",
    "from keras import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(sents_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(vocab)+1, output_dim=8, input_length=maxlen))\n",
    "model.add(LSTM(15))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 111334 samples, validate on 37112 samples\n",
      "Epoch 1/10\n",
      "  4992/111334 [>.............................] - ETA: 3:08 - loss: 0.6770 - acc: 0.6388"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-993ad90d3d54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit(X_train, y_train, batch_size=128, epochs=10,\n\u001b[0;32m----> 2\u001b[0;31m           validation_data=[X_valid, y_valid])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=128, epochs=10,\n",
    "          validation_data=[X_valid, y_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте теперь попробуем сделать предсказание этой моделью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = []\n",
    "gold = tokenizer.tokenize(data[3])\n",
    "for sent in gold:\n",
    "    sample += normalize(sent)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = []\n",
    "sents = []\n",
    "for word in sample:\n",
    "    if word not in word2id:\n",
    "        continue\n",
    "    stack.append(word)\n",
    "    vec = [word2id[w] for w in stack]\n",
    "    vec = pad_sequences([vec], maxlen)\n",
    "    pred = model.predict(vec)\n",
    "    if pred[0][0] > 0.7:\n",
    "        sents.append(stack)\n",
    "        stack = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
