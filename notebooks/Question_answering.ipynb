{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вопросно-ответные текста и понимание текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QA - одна из самых модных тем в Nlp. Часто ещё используют термин reading comprehension (понимание текста), но разницу понять очень сложно. За последнее время вышло много вопросно-ответных датасетов: \n",
    "\n",
    "Другие задачи тоже уже пробуют переделывать под формат вопросно-ответных систем.\n",
    "\n",
    "Самый популярный датасет - SQUAD от Стэнфорда. На нем тестируют все самые новые нейронки (BERT например). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем обучить какую-нибудь нейронку на этих данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Dropout, Dense, Activation, CuDNNLSTM\n",
    "from keras.layers import LSTM, Bidirectional,Input\n",
    "from keras.layers import concatenate\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import CuDNNLSTM, Dense, Bidirectional, Conv1D, MaxPooling1D, Dropout, GlobalAveragePooling1D, LSTM\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Layer  \n",
    "from keras import initializers, regularizers, constraints  \n",
    "from keras import backend as K\n",
    "\n",
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import keras\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = json.load(open('train-v2.0.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = json.load(open('dev-v2.0.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не будем заморачиваться с нормализацией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [word for word in words if word and word]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет устроен так - есть тексты из википедии, к какому-то параграфу этого текста задан вопрос и из этого же параграфа извлечен ответ. В версии 2.0 добавились также вопросы, на которые нет ответа, что усложняет задачу для модели, но мы пока будет игнорировать такие вопросы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого ответа даны начало и конец (индексы) в соответствующем параграфе. На этих индексах мы и будем обучаться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но для начала нужно все предобработать. Чтобы скоратить время обучения предобучим fastext на всех параграфах и вопросах (можно взять готовую модель получше)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = []\n",
    "questions = []\n",
    "\n",
    "starts = []\n",
    "ends = []\n",
    "\n",
    "for instance in train['data']:\n",
    "    for paragraph in instance['paragraphs']:\n",
    "        context = paragraph['context']\n",
    "        corpus.append(normalize(context))\n",
    "        \n",
    "        for qas in paragraph['qas']:\n",
    "            question = qas['question']\n",
    "            \n",
    "            if qas['is_impossible']:\n",
    "                continue\n",
    "            \n",
    "            for answer in qas['answers']:\n",
    "                start = answer['answer_start']\n",
    "                end = len(answer['text'])\n",
    "                contexts.append(normalize(context))\n",
    "                questions.append(normalize(question))\n",
    "                starts.append(start)\n",
    "                ends.append(end)\n",
    "\n",
    "                corpus.append(normalize(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для отложенной выборки сохраним исходные параграфы и индексы вопросов. Они пригодятся для тестирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts_dev = []\n",
    "questions_dev = []\n",
    "\n",
    "starts_dev = []\n",
    "ends_dev = []\n",
    "ids = []\n",
    "imporssible_ids = []\n",
    "raw_context = []\n",
    "for instance in dev['data']:\n",
    "    for paragraph in instance['paragraphs']:\n",
    "        context = paragraph['context']\n",
    "        corpus.append(normalize(context))\n",
    "        for qas in paragraph['qas']:\n",
    "            if qas['is_impossible']:\n",
    "                imporssible_ids.append(qas['id'])\n",
    "                continue\n",
    "            \n",
    "            question = qas['question']\n",
    "            \n",
    "            for answer in qas['answers']:\n",
    "                \n",
    "                start = answer['answer_start']\n",
    "                end = len(answer['text'])\n",
    "                contexts_dev.append(normalize(context))\n",
    "                questions_dev.append(normalize(question))\n",
    "                starts_dev.append(start)\n",
    "                ends_dev.append(end)\n",
    "                corpus.append(normalize(question))\n",
    "                ids.append(qas['id'])\n",
    "                raw_context.append(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем фастекст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = gensim.models.FastText(corpus, size=200, sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь построим словарь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "\n",
    "for context in contexts:\n",
    "    vocab.update(context)\n",
    "\n",
    "for question in questions:\n",
    "    vocab.update(question)\n",
    "    \n",
    "for context in contexts_dev:\n",
    "    vocab.update(context)\n",
    "\n",
    "for question in questions_dev:\n",
    "    vocab.update(question)\n",
    "\n",
    "id2word = {i+1:word for i, word in enumerate(vocab)}\n",
    "word2id = {word:i for i, word in id2word.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим матрицу со эмбеддингами всех слова. Потом подадим её к Embedding слой нейронки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.zeros((len(vocab) + 1, 200))\n",
    "\n",
    "for i in range(1, len(vocab)+1):\n",
    "    try:\n",
    "        embeddings[i] = ft[id2word[i]]\n",
    "    except KeyError:\n",
    "        embeddings[i] = np.random.rand((200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь преодразуем все слова индексы и привидем все к одной длине (максимальной)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## КОНТЕКСТ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts_le = [[word2id[word] for word in context] for context in contexts]\n",
    "max_len = max([len(c) for c in contexts])\n",
    "\n",
    "X_train_context = pad_sequences(contexts_le, max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts_le_dev = [[word2id.get(word, 0) for word in context] for context in contexts_dev]\n",
    "\n",
    "X_dev_context = pad_sequences(contexts_le_dev, max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ВОПРОС"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_le = [[word2id[word] for word in question] for question in questions]\n",
    "max_len_q = max([len(c) for c in questions])\n",
    "\n",
    "X_train_question = pad_sequences(questions_le, max_len_q, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_le_dev = [[word2id.get(word, 0) for word in question] for question in questions_dev]\n",
    "X_dev_question = pad_sequences(questions_le_dev, max_len_q, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим параметры для нейронки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)+1\n",
    "embedding_vector_length = 200\n",
    "\n",
    "max_span_begin = np.max(starts)\n",
    "max_span_end = np.max(ends)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "starts = np.array(starts)  \n",
    "ends = np.array(ends) \n",
    "\n",
    "starts_dev = np.array(starts_dev) \n",
    "ends_dev = np.array(ends_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь самая сложная часть. У нашей нейронки будет два входа и два выхода. \n",
    "\n",
    "**Первый вход** - для параграфа. Он будет эбмедиться и прогоняться через LSTM.\n",
    "\n",
    "**Второй вход** - для вопроса. Он будет эбмедиться и прогоняться через LSTM.\n",
    "\n",
    "**Конкатенация** - выходы с двух входов скливаются в один. \n",
    "\n",
    "**Первый выход** - для начального индекса. Тут будет классификация из N классов, где N - это максимальный индекс в ответах (на самом деле честнее делать просто длину параграфа)\n",
    "\n",
    "**Второй выход** - для длинны ответа. Тут будет классификация из N классов, где N - это максимальная длина ответа (на самом деле честнее делать просто длину параграфа)\n",
    "\n",
    "Так как мы решаем задачу классификации - лосс **categorical_crossentropy** (sparse потому что мы не энкодили в ohe вектора)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "context_input (InputLayer)      (None, 650)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ques_input (InputLayer)         (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 650, 200)     22157600    context_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 40, 200)      22157600    ques_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 650, 100)     100800      embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 40, 100)      100800      embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 650, 100)     0           bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 40, 100)      0           bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 690, 100)     0           dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 20)           8960        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 20)           8960        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "start (Dense)                   (None, 3127)         65667       bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "end (Dense)                     (None, 240)          5040        bidirectional_10[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 44,605,427\n",
      "Trainable params: 290,227\n",
      "Non-trainable params: 44,315,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Первый вход\n",
    "context_input = Input(shape=(max_len, ), name='context_input')\n",
    "emb_c = Embedding(input_dim=vocab_size, output_dim=200, weights=[embeddings], \n",
    "              input_length=max_len, trainable=False)(context_input)\n",
    "\n",
    "lstm_out_c = Bidirectional(CuDNNLSTM(50,  return_sequences=True,))(emb_c)\n",
    "drop_1 = Dropout(0.1)(lstm_out_c)\n",
    "\n",
    "# Второй вход\n",
    "ques_input = Input(shape=(max_len_q, ), name='ques_input')\n",
    "emb_q = Embedding(input_dim=vocab_size, output_dim=200, weights=[embeddings], \n",
    "              input_length=max_len_q, trainable=False)(ques_input)\n",
    "lstm_out_q = Bidirectional(CuDNNLSTM(50,return_sequences=True,) )(emb_q)\n",
    "drop_2 = Dropout(0.1)(lstm_out_q)\n",
    "\n",
    "# merger model\n",
    "merge_layer = concatenate([drop_1, drop_2], axis=1)\n",
    "biLSTM_s = Bidirectional(CuDNNLSTM(10,))(merge_layer)\n",
    "biLSTM_e = Bidirectional(CuDNNLSTM(10,))(merge_layer)\n",
    "\n",
    "# Выход 1\n",
    "softmax_1 = Dense(max_span_begin+1, activation='softmax', name='start')(biLSTM_s)\n",
    "\n",
    "# Выход 2\n",
    "softmax_2 = Dense(max_span_end+1, activation='softmax', name='end')(biLSTM_e)\n",
    "\n",
    "model = Model(inputs=[context_input, ques_input], outputs=[softmax_1, softmax_2])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем обучаться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 86821 samples, validate on 20302 samples\n",
      "Epoch 1/100\n",
      "86821/86821 [==============================] - 51s 591us/step - loss: 12.0245 - start_loss: 7.4201 - end_loss: 4.6044 - start_acc: 0.0118 - end_acc: 0.0347 - val_loss: 10.9409 - val_start_loss: 6.9387 - val_end_loss: 4.0022 - val_start_acc: 0.0230 - val_end_acc: 0.0428\n",
      "Epoch 2/100\n",
      "86821/86821 [==============================] - 50s 577us/step - loss: 10.7175 - start_loss: 6.7949 - end_loss: 3.9226 - start_acc: 0.0256 - end_acc: 0.0769 - val_loss: 10.7246 - val_start_loss: 6.8200 - val_end_loss: 3.9046 - val_start_acc: 0.0230 - val_end_acc: 0.0735\n",
      "Epoch 3/100\n",
      "86821/86821 [==============================] - 50s 578us/step - loss: 10.6130 - start_loss: 6.7294 - end_loss: 3.8836 - start_acc: 0.0256 - end_acc: 0.0817 - val_loss: 10.6893 - val_start_loss: 6.8017 - val_end_loss: 3.8876 - val_start_acc: 0.0230 - val_end_acc: 0.0735\n",
      "Epoch 4/100\n",
      "28672/86821 [========>.....................] - ETA: 30s - loss: 10.6012 - start_loss: 6.7158 - end_loss: 3.8853 - start_acc: 0.0266 - end_acc: 0.0809"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-92749f8b35f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m model.fit(training_data[0], training_data[1], batch_size=1024,  epochs=100, shuffle=True,\n\u001b[0;32m---> 13\u001b[0;31m           validation_data=(validation_data[0], validation_data[1]))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "validation_data=({'context_input': X_dev_context,\n",
    "                  'ques_input':X_dev_question}, \n",
    "                 {'start': starts_dev,\n",
    "                  'end': ends_dev})\n",
    "\n",
    "\n",
    "training_data=({'context_input': X_train_context,\n",
    "                'ques_input':X_train_question}, \n",
    "                 {'start': starts,\n",
    "                  'end': ends})\n",
    "\n",
    "model.fit(training_data[0], training_data[1], batch_size=1024,  epochs=100, shuffle=True,\n",
    "          validation_data=(validation_data[0], validation_data[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как метрики не очень подходящие можно оценить все с помощью evaluation скрипта от SQuAd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(validation_data[0], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предиктим распределения для каждого выхода. Делаем argmax() чтобы достать самый вероятный класс (индекс)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starts_pred = preds[0].argmax(axis=1)\n",
    "ends_pred = preds[1].argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По индексам достаем текст из нетронутых текстов из отложенной выборки. Для плохих вопросов, просто добавляем пустые строки (что на самом деле нечестно)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = {}\n",
    "for i in range(len(starts_pred)):\n",
    "    cont = raw_context[i]\n",
    "    span = cont[starts_pred[i]:starts_pred[i]+ends_pred[i]]\n",
    "    pred_dict[ids[i]] = span\n",
    "\n",
    "for idx in imporssible_ids:\n",
    "    pred_dict[idx] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(pred_dict, open('prediction.json', 'w'),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем скрипт."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 evaluate-v2.0.py dev-v2.0.json prediction.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import CuDNNLSTM, Dense, Bidirectional, Conv1D, MaxPooling1D, Dropout, GlobalAveragePooling1D, LSTM\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Layer  \n",
    "from keras import initializers, regularizers, constraints  \n",
    "from keras import backend as K\n",
    "\n",
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
