{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вопросно-ответные текста и понимание текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QA - одна из самых модных тем в Nlp. Часто ещё используют термин reading comprehension (понимание текста), но разницу понять очень сложно. За последнее время вышло много крутых и сложных вопросно-ответных датасетов (например, https://stanfordnlp.github.io/coqa/)\n",
    "\n",
    "Другие задачи тоже уже пробуют переделывать под формат вопросно-ответных систем - https://decanlp.com/\n",
    "\n",
    "Самый популярный датасет - SQUAD от Стэнфорда (https://rajpurkar.github.io/SQuAD-explorer/). На нем тестируют все самые новые нейронки (BERT например). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем обучить какую-нибудь нейронку на этих данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Dropout, Dense, Activation, CuDNNLSTM\n",
    "from keras.layers import LSTM, Bidirectional,Input\n",
    "from keras.layers import concatenate\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import CuDNNLSTM, Dense, Bidirectional, Conv1D, MaxPooling1D, Dropout, GlobalAveragePooling1D, LSTM\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Layer  \n",
    "from keras import initializers, regularizers, constraints  \n",
    "from keras import backend as K\n",
    "\n",
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import keras\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = json.load(open('train-v2.0.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = json.load(open('dev-v2.0.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не будем заморачиваться с нормализацией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [word for word in words if word and word]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет устроен так - есть тексты из википедии, к какому-то параграфу этого текста задан вопрос и из этого же параграфа извлечен ответ. В версии 2.0 добавились также вопросы, на которые нет ответа, что усложняет задачу для модели, но мы пока будет игнорировать такие вопросы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого ответа даны начало и конец (индексы) в соответствующем параграфе. На этих индексах мы и будем обучаться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но для начала нужно все предобработать. Чтобы скоратить время обучения предобучим fastext на всех параграфах и вопросах (можно взять готовую модель получше)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = []\n",
    "questions = []\n",
    "\n",
    "starts = []\n",
    "ends = []\n",
    "\n",
    "for instance in train['data']:\n",
    "    for paragraph in instance['paragraphs']:\n",
    "        context = paragraph['context']\n",
    "        corpus.append(normalize(context))\n",
    "        \n",
    "        for qas in paragraph['qas']:\n",
    "            question = qas['question']\n",
    "            \n",
    "            if qas['is_impossible']:\n",
    "                continue\n",
    "            \n",
    "            for answer in qas['answers']:\n",
    "                start = answer['answer_start']\n",
    "                end = len(answer['text'])\n",
    "                contexts.append(normalize(context))\n",
    "                questions.append(normalize(question))\n",
    "                starts.append(start)\n",
    "                ends.append(end)\n",
    "\n",
    "                corpus.append(normalize(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для отложенной выборки сохраним исходные параграфы и индексы вопросов. Они пригодятся для тестирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts_dev = []\n",
    "questions_dev = []\n",
    "\n",
    "starts_dev = []\n",
    "ends_dev = []\n",
    "ids = []\n",
    "imporssible_ids = []\n",
    "raw_context = []\n",
    "for instance in dev['data']:\n",
    "    for paragraph in instance['paragraphs']:\n",
    "        context = paragraph['context']\n",
    "        corpus.append(normalize(context))\n",
    "        for qas in paragraph['qas']:\n",
    "            if qas['is_impossible']:\n",
    "                imporssible_ids.append(qas['id'])\n",
    "                continue\n",
    "            \n",
    "            question = qas['question']\n",
    "            \n",
    "            for answer in qas['answers']:\n",
    "                \n",
    "                start = answer['answer_start']\n",
    "                end = len(answer['text'])\n",
    "                contexts_dev.append(normalize(context))\n",
    "                questions_dev.append(normalize(question))\n",
    "                starts_dev.append(start)\n",
    "                ends_dev.append(end)\n",
    "                corpus.append(normalize(question))\n",
    "                ids.append(qas['id'])\n",
    "                raw_context.append(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем фастекст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = gensim.models.FastText(corpus, size=200, sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь построим словарь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "\n",
    "for context in contexts:\n",
    "    vocab.update(context)\n",
    "\n",
    "for question in questions:\n",
    "    vocab.update(question)\n",
    "    \n",
    "for context in contexts_dev:\n",
    "    vocab.update(context)\n",
    "\n",
    "for question in questions_dev:\n",
    "    vocab.update(question)\n",
    "\n",
    "id2word = {i+1:word for i, word in enumerate(vocab)}\n",
    "word2id = {word:i for i, word in id2word.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим матрицу со эмбеддингами всех слова. Потом подадим её к Embedding слой нейронки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.zeros((len(vocab) + 1, 200))\n",
    "\n",
    "for i in range(1, len(vocab)+1):\n",
    "    try:\n",
    "        embeddings[i] = ft[id2word[i]]\n",
    "    except KeyError:\n",
    "        embeddings[i] = np.random.rand((200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь преодразуем все слова индексы и привидем все к одной длине (максимальной)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## КОНТЕКСТ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts_le = [[word2id[word] for word in context] for context in contexts]\n",
    "max_len = max([len(c) for c in contexts])\n",
    "\n",
    "X_train_context = pad_sequences(contexts_le, max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts_le_dev = [[word2id.get(word, 0) for word in context] for context in contexts_dev]\n",
    "\n",
    "X_dev_context = pad_sequences(contexts_le_dev, max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ВОПРОС"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_le = [[word2id[word] for word in question] for question in questions]\n",
    "max_len_q = max([len(c) for c in questions])\n",
    "\n",
    "X_train_question = pad_sequences(questions_le, max_len_q, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_le_dev = [[word2id.get(word, 0) for word in question] for question in questions_dev]\n",
    "X_dev_question = pad_sequences(questions_le_dev, max_len_q, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим параметры для нейронки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)+1\n",
    "embedding_vector_length = 200\n",
    "\n",
    "max_span_begin = np.max(starts)\n",
    "max_span_end = np.max(ends)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starts = np.array(starts)  \n",
    "ends = np.array(ends) \n",
    "\n",
    "starts_dev = np.array(starts_dev) \n",
    "ends_dev = np.array(ends_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь самая сложная часть. У нашей нейронки будет два входа и два выхода. \n",
    "\n",
    "**Первый вход** - для параграфа. Он будет эбмедиться и прогоняться через LSTM.\n",
    "\n",
    "**Второй вход** - для вопроса. Он будет эбмедиться и прогоняться через LSTM.\n",
    "\n",
    "**Конкатенация** - выходы с двух входов скливаются в один. \n",
    "\n",
    "**Первый выход** - для начального индекса. Тут будет классификация из N классов, где N - это максимальный индекс в ответах (на самом деле честнее делать просто длину параграфа)\n",
    "\n",
    "**Второй выход** - для длинны ответа. Тут будет классификация из N классов, где N - это максимальная длина ответа (на самом деле честнее делать просто длину параграфа)\n",
    "\n",
    "Так как мы решаем задачу классификации - лосс **categorical_crossentropy** (sparse потому что мы не энкодили в ohe вектора)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Первый вход\n",
    "context_input = Input(shape=(max_len, ), name='context_input')\n",
    "emb_c = Embedding(input_dim=vocab_size, output_dim=200, weights=[embeddings], \n",
    "              input_length=max_len, trainable=False)(context_input)\n",
    "\n",
    "lstm_out_c = Bidirectional(CuDNNLSTM(50,  return_sequences=True,))(emb_c)\n",
    "drop_1 = Dropout(0.1)(lstm_out_c)\n",
    "\n",
    "# Второй вход\n",
    "ques_input = Input(shape=(max_len_q, ), name='ques_input')\n",
    "emb_q = Embedding(input_dim=vocab_size, output_dim=200, weights=[embeddings], \n",
    "              input_length=max_len_q, trainable=False)(ques_input)\n",
    "lstm_out_q = Bidirectional(CuDNNLSTM(50,return_sequences=True,) )(emb_q)\n",
    "drop_2 = Dropout(0.1)(lstm_out_q)\n",
    "\n",
    "# merger model\n",
    "merge_layer = concatenate([drop_1, drop_2], axis=1)\n",
    "biLSTM_s = Bidirectional(CuDNNLSTM(10,))(merge_layer)\n",
    "biLSTM_e = Bidirectional(CuDNNLSTM(10,))(merge_layer)\n",
    "\n",
    "# Выход 1\n",
    "softmax_1 = Dense(max_span_begin+1, activation='softmax', name='start')(biLSTM_s)\n",
    "\n",
    "# Выход 2\n",
    "softmax_2 = Dense(max_span_end+1, activation='softmax', name='end')(biLSTM_e)\n",
    "\n",
    "model = Model(inputs=[context_input, ques_input], outputs=[softmax_1, softmax_2])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем обучаться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validation_data=({'context_input': X_dev_context,\n",
    "                  'ques_input':X_dev_question}, \n",
    "                 {'start': starts_dev,\n",
    "                  'end': ends_dev})\n",
    "\n",
    "\n",
    "training_data=({'context_input': X_train_context,\n",
    "                'ques_input':X_train_question}, \n",
    "                 {'start': starts,\n",
    "                  'end': ends})\n",
    "\n",
    "model.fit(training_data[0], training_data[1], batch_size=1024,  epochs=100, shuffle=True,\n",
    "          validation_data=(validation_data[0], validation_data[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как метрики не очень подходящие можно оценить все с помощью evaluation скрипта от SQuAd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(validation_data[0], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предиктим распределения для каждого выхода. Делаем argmax() чтобы достать самый вероятный класс (индекс)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starts_pred = preds[0].argmax(axis=1)\n",
    "ends_pred = preds[1].argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По индексам достаем текст из нетронутых текстов из отложенной выборки. Для плохих вопросов, просто добавляем пустые строки (что на самом деле нечестно)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = {}\n",
    "for i in range(len(starts_pred)):\n",
    "    cont = raw_context[i]\n",
    "    span = cont[starts_pred[i]:starts_pred[i]+ends_pred[i]]\n",
    "    pred_dict[ids[i]] = span\n",
    "\n",
    "for idx in imporssible_ids:\n",
    "    pred_dict[idx] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(pred_dict, open('prediction.json', 'w'),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем скрипт."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 evaluate-v2.0.py dev-v2.0.json prediction.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import CuDNNLSTM, Dense, Bidirectional, Conv1D, MaxPooling1D, Dropout, GlobalAveragePooling1D, LSTM\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Layer  \n",
    "from keras import initializers, regularizers, constraints  \n",
    "from keras import backend as K\n",
    "\n",
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
