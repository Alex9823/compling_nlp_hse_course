{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Семинар 7. Извлечение именованных сущностей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Именованные сущности - общий термин, который используют для обозначения какого-то множества слов (словосочетаний, последовательностей символов), которые представляют какой-то особый инетерес в контексте решаемой практической задачи и которые нужно отделить от остальных слов. Это могут быть классические имена, фамилии, названия организаций, городов, рек, гор, а могут быть и, например, денежные суммы, даты, статьи кодексов, номера телефонов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В многих случаях случаях можно написать регуляру и решиьть задачу на приемлимом уровне (например собрать все номера телефонов совсем несложно). Но когда сущности выраются нестандартным образом, постоянно расширяются и меняются по формам, задача становится очень сложной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К тому же в разных текстах употребляются разные сущности. Поэтому универсальных извлекателей сущностей нет. Есть только стандартные Персоны, Локации, Организации. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для английского удобно использовать spacy. Там сразу извлекаются сущности с хорошим качеством."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для русского (если не хочется ничего делать) можно использовать тэги из pymorphy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = morph.parse('Михаил')[0].tag\n",
    "print('Тэги - ', p)\n",
    "print('Name' in p) #тэг имени"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = morph.parse('Иванов')[0].tag\n",
    "print('Тэги - ', p)\n",
    "print('Surn' in p) #тэг фамилии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = morph.parse('Петрович')[0].tag\n",
    "print('Тэги - ', p)\n",
    "print('Patr' in p) #тэг отчества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = morph.parse('Москва')[0].tag\n",
    "print('Тэги - ', p)\n",
    "print('Geox' in p) #тэг локация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = morph.parse('Яндекс')[0].tag\n",
    "print('Тэги - ', p)\n",
    "print('Orgn' in p) #тэг организация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = morph.parse('')[0].tag\n",
    "print('Тэги - ', p)\n",
    "print('Orgn' in p) #тэг организация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работает не очень хорошо, но все равно лучше, чем ничего. Рядом стоящие слова одного тэга можно склеить в один. Или сначала собрать нграмы и если какое-то одно слово в нграмме принадлежит к какому-то типу сущности, то распространить его на весь нграм."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть пара библотек, специально предназначенных для этого. Например, natasha - https://github.com/natasha/natasha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Она основана на парсере yargy https://github.com/natasha/yargy и представляет собой набор готовых правил для извлечения некоторых сущностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (NamesExtractor,\n",
    "                     SimpleNamesExtractor,\n",
    "                     PersonExtractor,\n",
    "                     LocationExtractor,\n",
    "                     AddressExtractor,\n",
    "                     OrganisationExtractor,\n",
    "                     DatesExtractor,\n",
    "                     MoneyExtractor,\n",
    "                     MoneyRateExtractor,\n",
    "                     MoneyRangeExtractor)\n",
    "\n",
    "from natasha.markup import (show_markup_notebook as show_markup,\n",
    "                            format_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Влад Веселов. Петрович. Алиса. Студия Артемия Лебедева'\n",
    "\n",
    "extractor_per = NamesExtractor()\n",
    "matches = extractor_per(text)\n",
    "spans = [_.span for _ in matches]\n",
    "facts = [_.fact.as_json for _ in matches]\n",
    "show_markup(text, spans)\n",
    "# print(format_json(facts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Влад Веселов. Петрович. Алиса. Студия Артемия Лебедева'\n",
    "\n",
    "extractor_per = PersonExtractor()\n",
    "matches = extractor_per(text)\n",
    "spans = [_.span for _ in matches]\n",
    "facts = [_.fact.as_json for _ in matches]\n",
    "show_markup(text, spans)\n",
    "# print(format_json(facts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Более того в Москве, в районе Строгино. На реке Оке. В германии'\n",
    "\n",
    "extractor_loc = LocationExtractor()\n",
    "matches = extractor_loc(text)\n",
    "spans = [_.span for _ in matches]\n",
    "facts = [_.fact.as_json for _ in matches]\n",
    "show_markup(text, spans)\n",
    "# print(format_json(facts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'ФСБ. Московский государственный университет. Высшая школа экономика. ВШЭ. Mail.ru'\n",
    "\n",
    "extractor_org = OrganisationExtractor()\n",
    "matches = extractor_org(text)\n",
    "spans = [_.span for _ in matches]\n",
    "facts = [_.fact.as_json for _ in matches]\n",
    "show_markup(text, spans)\n",
    "# print(format_json(facts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'С 2015 по 2017 год. 16 апреля 1993 года. В субботу. 23.04.18'\n",
    "\n",
    "extractor_date = DatesExtractor()\n",
    "matches = extractor_date(text)\n",
    "spans = [_.span for _ in matches]\n",
    "facts = [_.fact.as_json for _ in matches]\n",
    "show_markup(text, spans)\n",
    "# print(format_json(facts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Он заплатил ему 300 рублей.\"\n",
    "\n",
    "extractor_money = MoneyExtractor()\n",
    "matches = extractor_money(text)\n",
    "spans = [_.span for _ in matches]\n",
    "facts = [_.fact.as_json for _ in matches]\n",
    "show_markup(text, spans)\n",
    "# print(format_json(facts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В yargy можно писать свои грамматики, подробнее про синтаксис можно почитать в: http://yargy.readthedocs.io/ru/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё есть томита-парсер, но с ним очень тяжело работать (никакого развития, скудная документация, закрытый код, никакого сообщества) https://tech.yandex.ru/tomita/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для русского state-of-the-art - библеотека от Ipavlov из Физтеха.\n",
    "https://github.com/deepmipt/ner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Она основана на BiLSTM-CRF (нейронки) и скорее всего просто так не поставится и использовать её будет трудновато (без GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ner\n",
    "extractor = ner.Extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(extractor('ФСБ. Московский государственный университет. \\\n",
    "               Высшая школа экономика. ВШЭ. Mail.ru'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тэги у них стандартные (PER, LOC, ORG). Если нужно выделять какие-то другие сущности, то нужно размечать свой корпус и обучать свою модель. Даже если нужны только эти категории, может потребоваться новый корпус и модель, так как модели обучены на каких-то общих корпусах (новостных, вики), а на практике приходится работать с какими-то специфичными вещами (рецептами, резюме, судебными решениями)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Размечать корпус дорого и сложно, поэтому люди постоянно пытаются придумать способы сделать NER без разметки корпуса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одна из самых старых идей (обычно ссылаются на работу Hearst как первую - http://www.aclweb.org/anthology/C92-2082) это бутстреппинг. Суть в том, чтобы с помощью какого-то точного паттерна извлечь набор сущностей, а потом с помощью этих сущностей собрать новые паттерны (и повторить всё заново)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import string\n",
    "import numpy as np\n",
    "from pymorphy2 import MorphAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем небольшой (100 тыс предложений) корпус из википедии и попробуем извлечь какой-то набор организаций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = Counter()\n",
    "\n",
    "# паттерн \"компания X+\"\n",
    "# скорее всего, слово с заглавной буквы после слова компания - название компании\n",
    "\n",
    "pattern = 'компания ([А-ЯЁA-Z«][\\w\\-\\.»]+(?: [А-ЯЁA-Z][\\w\\-\\w\\.»]+)*)'\n",
    "new = Counter()\n",
    "\n",
    "for line in open('sentences_100k_wiki.txt'):\n",
    "    candidates.update(re.findall(pattern, line))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "candidates.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом можно извлекать и более узкие вещи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = Counter()\n",
    "\n",
    "\n",
    "pattern = 'прозвищу ([А-ЯЁA-Z][\\w-]+(?: [А-ЯЁA-Z][\\w-]+)*)'\n",
    "new = Counter()\n",
    "\n",
    "for line in open('sentences_100k_wiki.txt'):\n",
    "    candidates.update(re.findall(pattern, line))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно воспользоваться паттерном - \"X и Y\", чтобы найти однотипиные слова. Только сначала нужно задать небольшой список примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = Counter(['Париж', \"Берлин\", \"Москва\"])\n",
    "pattern = '(?:{}) и ([А-ЯЁ][\\w-]+)'\n",
    "new = Counter()\n",
    "seen = set()\n",
    "\n",
    "# пройдем по корпусу несколько раз,\n",
    "# каждый раз добавляя новые примеры\n",
    "# чтобы не искать по одному и тому же слову,\n",
    "# сделаем словарь посещенных\n",
    "# чтобы не включать мусор в поиск, будем искать только по топ-15\n",
    "for i in range(15):\n",
    "    print(\"Итерация - \", i)\n",
    "    print(\"Собрано - \", len(candidates))\n",
    "    cands = set([cand for cand,_ in candidates.most_common(25)])\n",
    "    cands -= seen\n",
    "    \n",
    "    if not cands:\n",
    "        print('Ничего не найдено!')\n",
    "        break\n",
    "    \n",
    "    for line in open('sentences_100k_wiki.txt'):\n",
    "        new.update(re.findall(pattern.format('|'.join(cands)), line))\n",
    "        seen |= cands\n",
    "        candidates += new\n",
    "        new = Counter()\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получились не совсем города, но под категорию LOC подходит. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "candidates.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно также поискать частотные паттерны, в которых встречается какой-то тип сущности. Используем тэги и pymorphy и достанем 2 предыдущих и два последующих слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()\n",
    "patterns = Counter()\n",
    "\n",
    "before_patterns = Counter()\n",
    "\n",
    "after_patterns = Counter()\n",
    "\n",
    "for line in open('sentences_100k_wiki.txt'):\n",
    "\n",
    "    words = ['<START>', ] + line.split() + ['<END>']\n",
    "    tags = [morph.parse(word.strip(string.punctuation))[0].tag for word in words]\n",
    "    tags = ['LOC' if 'Geox' in tag else '' for tag in tags]\n",
    "    inds = []\n",
    "    start = None\n",
    "    end = None\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag == 'LOC':\n",
    "            if start:\n",
    "                end = i\n",
    "            else:\n",
    "                start = i\n",
    "\n",
    "        else:\n",
    "            if start:\n",
    "                end = i\n",
    "                inds.append((start, end))\n",
    "                start, end = None, None            \n",
    "\n",
    "    for ind in inds:\n",
    "        start, end = ind\n",
    "        before = max(0, start-2)\n",
    "        after = end+3\n",
    "        before_context = ' '.join(words[before:start])\n",
    "        after_context = ' '.join(words[end+1:after])\n",
    "        patterns.update([(before_context, after_context)])\n",
    "        before_patterns.update([before_context])\n",
    "        after_patterns.update([after_context])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бустреппинг предполагает поочередный поиск патернов и сущностей. Мы этого делать не будет, так как в этом случае появляется проблема с мусорными примерами и паттернами. Оценивание мусорности сущности или паттерна - сложная задача."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Даже единичными паттернами можно набрать какое-то количесто примеров и просто разметить ими корпус. А уже на нем обучить модель, которая захватит всякие контекстуальные признаки и обобщит разметку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_text(text, gazzeteer, tag):\n",
    "    labels = []\n",
    "    text = re.sub('  +', ' ', text)\n",
    "\n",
    "    for word in gazzeteer:\n",
    "        start = text.find(word)\n",
    "        if start >= 0:\n",
    "            labels.append((start, start+len(word)))\n",
    "    \n",
    "    \n",
    "    words = text.split()\n",
    "    if not labels:\n",
    "        return (False, [(word, 'O') for word in words])\n",
    "    \n",
    "    spans = []\n",
    "    i = 0\n",
    "    for word in words:\n",
    "        strip_word_right = word.rstrip(string.punctuation)\n",
    "        strip_word_left = word.lstrip(string.punctuation)\n",
    "\n",
    "        spans.append((i, i+len(word)-len(strip_word_left), i+len(word), i+len(strip_word_right)))\n",
    "        i += len(word)\n",
    "        i += 1\n",
    "\n",
    "    tags = []\n",
    "    for span in spans:\n",
    "        for label in labels:\n",
    "            if (span[0] >= label[0] or span[1] >= label[0]) \\\n",
    "              and (span[2] <= label[1] or span[3] <= label[1]):\n",
    "                tags.append(tag)\n",
    "                break\n",
    "        else:\n",
    "            tags.append('O')\n",
    "    bio_tags = []\n",
    "    inside = False\n",
    "    for tag in tags:\n",
    "        if tag != 'O':\n",
    "            if inside:\n",
    "                bio_tags.append(tag+'-I')\n",
    "            else:\n",
    "                bio_tags.append(tag+'-B')\n",
    "                inside = True\n",
    "        else:\n",
    "            bio_tags.append(tag)\n",
    "            inside = False\n",
    "    \n",
    "    if any([tag!='O' for tag in bio_tags]):\n",
    "        return (True, list(zip(words, bio_tags)))\n",
    "    else:\n",
    "        return (False, list(zip(words, bio_tags)))\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orgs = set([org for org, c in candidates.most_common(500)]) - set(['Украины', \"ООО\", \"Гудзонова\", \"No\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'SpaceX представил.'\n",
    "\n",
    "label_text(text, orgs, 'ORG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = []\n",
    "negative = []\n",
    "\n",
    "for line in open('sentences_100k_wiki.txt'):\n",
    "    labeled = label_text(line, orgs, 'ORG')\n",
    "    if labeled[0]:\n",
    "        positive.append(labeled[1])\n",
    "    else:\n",
    "        if np.random.randint(10) > 8:\n",
    "            negative.append(labeled[1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive = []\n",
    "# negative = []\n",
    "\n",
    "# locs = set([loc for loc, c in candidates.most_common(500)])\n",
    "# for line in open('sentences_100k_wiki.txt'):\n",
    "#     labeled = label_text(line, locs, 'LOC')\n",
    "#     if labeled[0]:\n",
    "#         positive.append(labeled[1])\n",
    "#     else:\n",
    "#         if np.random.randint(10) > 8:\n",
    "#             negative.append(labeled[1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы модель обобщалась обучим fasttext и обучим модель на векторах слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import string\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[word.strip(string.punctuation) for word in line.lower().split()] for line in open('sentences_100k_wiki.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_text = gensim.models.FastText(data, max_vocab_size=120000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(word, model, dim):\n",
    "    word = word.lower()\n",
    "    try:\n",
    "        v = model[word].reshape(1,-1)\n",
    "    except (KeyError, ValueError):\n",
    "        v = np.zeros((dim)).reshape(1,-1)\n",
    "    \n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_matrix(sents, model, dim):\n",
    "    size = sum((len(s) for s in sents))\n",
    "    \n",
    "    #создадим большую матрицу для всех слов\n",
    "    X = np.zeros((size, (dim*2)+3))\n",
    "    y = np.zeros(size, dtype='object')\n",
    "    \n",
    "    ind = 0\n",
    "    # пройдем по всем предложениям\n",
    "    for i, sent in enumerate(sents):\n",
    "        # в каждом предложении каждому слову препишем\n",
    "        # его вектор и вектор предыдушего слова\n",
    "        # также добавим признак начала предложения\n",
    "        # и предыдущего тэга\n",
    "        for j, word in enumerate(sent):\n",
    "            word, tag = word\n",
    "            \n",
    "            if j: # если не начало\n",
    "                prev_word, prev_tag = sent[j-1]\n",
    "                if '-B' in prev_tag:\n",
    "                    prev_tag = [0, 1]\n",
    "                elif '-I' in prev_tag:\n",
    "                    prev_tag = [1, 0]\n",
    "                else:\n",
    "                    prev_tag = [0, 0]\n",
    "                prev_vec = get_embedding(prev_word, model, dim)\n",
    "                start_sent = 0\n",
    "\n",
    "            else: # если начало\n",
    "                # нулевой вектор для предыдущего первого слова\n",
    "                prev_vec = np.zeros((dim)).reshape(1, -1) \n",
    "                start_sent = 1\n",
    "                prev_tag = [0, 0]\n",
    "\n",
    "            vec =  get_embedding(word, model, dim)\n",
    "\n",
    "            y[ind] = tag\n",
    "\n",
    "            X[ind] = np.concatenate([vec, prev_vec, [prev_tag], [[start_sent]]], axis=1)\n",
    "            \n",
    "            ind += 1\n",
    "\n",
    "    return X, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_text_matrix(positive[:10000], fast_text, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, valid_X, train_y, valid_y = train_test_split(X, y,random_state=1, stratify=y)\n",
    "clf = LogisticRegression(C=100, class_weight='balanced')\n",
    "clf.fit(train_X, train_y)\n",
    "preds = clf.predict(valid_X)\n",
    "print(classification_report(valid_y, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для предсказания нужно написать отдельную функцию, так как у нас нет изначально нет никакой информации о тэгах. Нужно предсказывать тэг для каждого слова отдельно и передавать его дальше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_pred(sent, model, dim, clf):\n",
    "    preds = []\n",
    "    words = []\n",
    "    pred_tags = []\n",
    "    words.append(sent[0])\n",
    "    vec =  get_embedding(sent[0], model, dim)\n",
    "    prev_vec = np.zeros((dim)).reshape(1, -1)\n",
    "    start_sent = 1\n",
    "    prev_tag = [0, 0]\n",
    "    v = np.concatenate([vec, prev_vec, [prev_tag], [[start_sent]]], axis=1)\n",
    "    pred = clf.predict(v)[0]\n",
    "    pred_tags.append(pred)\n",
    "\n",
    "    for j, word in enumerate(sent[1:]):\n",
    "\n",
    "        words.append(word)\n",
    "        prev_vec = vec\n",
    "        vec =  get_embedding(word, model, dim)\n",
    "        start_sent = 0\n",
    "        \n",
    "        if '-B' in pred:\n",
    "            prev_tag = [0, 1]\n",
    "        elif '-I' in pred:\n",
    "            prev_tag = [1, 0]\n",
    "        else:\n",
    "            prev_tag = [0, 0]\n",
    "        v = np.concatenate([vec, prev_vec, [prev_tag], [[start_sent]]], axis=1)\n",
    "        pred = clf.predict(v)[0]\n",
    "\n",
    "        pred_tags.append(pred)\n",
    "\n",
    "    \n",
    "    return list(zip(words, pred_tags))\n",
    "\n",
    "def get_preds(sents, model, dim, clf):\n",
    "    \n",
    "    preds = []\n",
    "\n",
    "    for i, sent in enumerate(sents):\n",
    "        pred = get_sent_pred(sent, model, dim, clf)\n",
    "        preds.append(pred)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [[word for word, tag in sent] for sent in negative[10000:20000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds = get_preds(sents, fast_text, 100, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[pred for pred in preds if any([x[1] != 'O' for x in pred])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что-то таким образом предсказывается, но чтобы довести это всё до приличного состояния нужно ещё много улучшений. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть даже фреймворк для такой полуручной разметки данных - https://hazyresearch.github.io/snorkel/ и вообще weak supervision достаточно популярная тема."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
